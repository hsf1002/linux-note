# linux性能优化



![img](https://static001.geekbang.org/resource/image/9e/7a/9ee6c1c5d88b0468af1a3280865a6b7a.png)



![img](https://static001.geekbang.org/resource/image/0f/ba/0faf56cd9521e665f739b03dd04470ba.png)

## CPU性能篇

### 到底应该怎么理解平均负载？

```
13:11  up 124 days, 3 users, load averages: 1.99 2.10 2.33
当前时间  系统登录时长  登录账户个数  1、5、15分钟CPU的平均负载
```

##### 平均负载

指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数，它和 CPU 使用率并没有直接关系

* 可运行状态的进程：正在使用 CPU 或者正在等待 CPU 的进程，ps 命令看到的处于 R 状态（Running 或 Runnable）的进程
* 不可中断状态的进程：正处于内核态关键流程中的进程，不可被打断，比如最常见的是等待硬件设备的 I/O 响应， ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程，不可中断状态实际上是系统对进程和硬件设备的一种保护机制

最理想的是每个 CPU 上都刚好运行着一个进程，每个 CPU 都得到充分利用。当平均负载为 2 时，意味着：

* 在只有 2 个 CPU 的系统上，意味着所有的 CPU 都刚好被完全占用
* 在 4 个 CPU 的系统上，意味着 CPU 有 50% 的空闲
* 在只有 1 个 CPU 的系统中，则意味着有一半的进程竞争不到 CPU

##### 平均负载为多少时合理

查看本机CPU数量：

```
grep 'model name' /proc/cpuinfo | wc -l
4
```

1、5、15分钟的平均负载，提供了分析系统负载趋势的数据来源，可以更全面、更立体地理解目前的负载状况，当平均负载高于 CPU 数量 70% 的时候，你就应该分析排查负载高的问题了

##### 平均负载与 CPU 使用率

平均负载不仅包括了正在使用 CPU 的进程，还包括等待 CPU 和等待 I/O 的进程。而 CPU 使用率，是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应。比如：

* CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时两者是一致的
* I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高
* 大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高

##### 案例分析

分析工具：

```
apt install stress sysstat
```

* stress：一个 Linux 系统压力测试工具，用作异常进程模拟平均负载升高的场景

* sysstat：包含了常用的 Linux 性能工具，用来监控和分析系统的性能。这个包的两个命令 mpstat 和 pidstat
  * mpstat：一个常用的多核 CPU 性能分析工具，可实时查看每个 CPU 的性能指标以及所有 CPU 的平均指标
  * pidstat：一个常用的进程性能分析工具，用来实时查看进程的 CPU、内存、I/O 以及上下文切换等性能指标

场景一：CPU  密集型进程

```
// 第一个终端运行  stress  命令，模拟一个  CPU  使用率 100% 的场景
stress --cpu 1 --timeout 600

// 第二个终端运行 uptime 查看平均负载的变化情况
// -d 参数表示高亮显示变化的区域
watch -d uptime

// 第三个终端运行 mpstat 查看  CPU  使用率的变化情况
// -P ALL 表示监控所有CPU，后面数字5表示间隔5秒后输出一组数据
mpstat -P ALL 5

平均负载的升高正是由于 CPU 使用率为 100% 

// 哪个进程导致了 CPU 使用率为 100% ？可以使用 pidstat 来查询
// 间隔5秒后输出一组数据
pidstat -u 5 1
```

场景二：I/O  密集型进程

```
// 第一个终端运行  stress  命令，但这次模拟  I/O  压力，即不停地执行  sync
stress -i 1 --timeout 600

// 第二个终端运行 uptime 查看平均负载的变化情况
watch -d uptime

// 第三个终端运行 mpstat 查看  CPU  使用率的变化情况
mpstat -P ALL 5

平均负载的升高正是由于 IO 使用率为 90% 

// 哪个进程导致了 IO 使用率为 90% ？可以使用 pidstat 来查询
pidstat -u 5 1
```

场景三：大量进程的场景

```
// 第一个终端运行  stress  命令，模拟创建了16个进程
stress -c 16 --timeout 600
 
// 第二个终端运行 uptime 查看平均负载的变化情况， 系统只有 4 个 CPU，明显比 16 个进程要少得多，因而，系统的 CPU 处于严重过载状态
watch -d uptime

// 第三个终端查看哪个进程占用资源，发现有16个进程分布在4个CPU上
pidstat -u 5 1
```

### 经常说的 CPU 上下文切换是什么意思？

##### CPU 上下文

用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做 CPU 上下文

##### CPU 上下文切换

先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。根据任务的不同，CPU 的上下文切换可以分为几个不同的场景，进程上下文切换、线程上下文切换以及中断上下文切换

##### 进程上下文切换

从用户态到内核态的转变，需要通过系统调用来完成。比如，当我们查看文件内容时，就需要多次系统调用来完成：首先调用 open() 打开文件，调用 read() 读取文件内容，调用 write() 将内容写到标准输出，最后再调用 close() 关闭文件。系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。一次系统调用的过程，其实是发生了两次 CPU 上下文切换。

* 进程上下文切换：从一个进程切换到另一个进程运行，进程的切换只能发生在内核态，每次上下文切换都需要几十纳秒到数微秒的 CPU 时间，这个时间相当可观
* 系统调用：一直是同一个进程在运行。通常称为特权模式切换，而不是上下文切换。但实际上，系统调用过程中，CPU 的上下文切换还是无法避免的

进程切换时才需要切换上下文，进程切换的原因：

* CPU时间片到了
* 系统资源不足导致进程被挂起
* 通过睡眠函数  sleep 这样的方法将自己主动挂起时
* 有优先级更高的进程运行时
* 发生硬件中断时

##### 线程上下文切换

线程与进程最大的区别在于，线程是调度的基本单位，而进程则是资源拥有的基本单位。所谓内核中的任务调度，实际上的调度对象是线程；而进程只是给线程提供了虚拟内存、全局变量等资源

* 当进程只有一个线程时，可以认为进程就等于线程
* 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的
* 线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的

线程的上下文切换其实就可以分为两种情况：

* 第一种： 前后两个线程属于不同进程。因为资源不共享，所以切换过程就跟进程上下文切换是一样
* 第二种：前后两个线程属于同一个进程。因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据

虽然同为上下文切换，但同进程内的线程切换，要比多进程间的切换消耗更少的资源

##### 中断上下文切换

中断处理会打断进程的正常调度和执行，转而调用中断处理程序。而在打断其他进程时，就需要将进程当前的状态保存下来，这样在中断结束后，进程仍然可以从原来的状态恢复运行。跟进程上下文不同，中断上下文切换并不涉及到进程的用户态，所以无需保存其用户态资源，只需要保存内核态中断服务程序执行所必需的状态。对同一个 CPU 来说，中断处理比进程拥有更高的优先级，所以中断上下文切换并不会与进程上下文切换同时发生。大部分中断处理程序都短小精悍，以便尽可能快的执行结束。跟进程上下文切换一样，中断上下文切换也需要消耗 CPU，切换次数过多也会耗费大量的 CPU，甚至严重降低系统的整体性能

* CPU 上下文切换，是保证 Linux 系统正常工作的核心功能之一，一般情况下不需要特别关注
* 过多的上下文切换，会把 CPU 时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，从而缩短进程真正运行的时间，导致系统的整体性能大幅下降

##### 查看系统的上下文切换情况

分析工具：

* vmstat：一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数

```
// 每隔5秒输出1组数据
vmstat 5
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0      0 7005360  91564 818900    0    0     0     0   25   33  0  0 100  0  0
 
r（running or runnable）：就绪队列的长度，也就是正在运行和等待 CPU 的进程数
b（blocked）：处于不可中断睡眠状态的进程数
in (interrupt)：每秒中断的次数
cs (context switch)：每秒上下文切换的次数
us (user CPU time)：用户空间占用CPU百分比
sy (system CPU time)：内核空间占用CPU百分比
ni (nice CPU time)：用户进程空间内改变过优先级的进程占用CPU百分比
id (idle)：空闲CPU百分比
wa (iowait)：等待输入输出的CPU时间百分比
hi (hardware irq)：硬件中断
si (software irq)：软件中断 
st (steal time)：实时
```

查看每个进程的详细情况，需要使用 pidstat。加上 -w 选项，就可以查看每个进程上下文切换的情况

```
// 每隔5秒输出1组数据
pidstat -w 5
Linux 4.15.0 (ubuntu)  09/23/18  _x86_64_  (2 CPU)

08:18:26      UID       PID   cswch/s nvcswch/s  Command
08:18:31        0         1      0.20      0.00  systemd
08:18:31        0         8      5.40      0.00  rcu_sched

cswch（voluntary context switches）：自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I/O、内存等系统资源不足时
nvcswch（non voluntary context switches）：非自愿上下文切换，是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时
```

##### 案例分析

分析工具：

* sysbench： 是一个多线程的基准测试工具，一般用来评估不同系统参数下的数据库负载情况

```
// 空闲系统的上下文切换次数，间隔1秒后输出1组数据
vmstat 1 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0      0 6984064  92668 830896    0    0     2    19   19   35  1  0 99  0  0
 
// 第一个终端里运行 sysbench 模拟系统多线程调度的瓶颈, 以10个线程运行5分钟的基准测试，模拟多线程切换的问题
sysbench --threads=10 --max-time=300 threads run

// 第二个终端运行 vmstat，观察上下文切换情况, 每隔1秒输出1组数据（需要Ctrl+C才结束）
vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 6  0      0 6487428 118240 1292772    0    0     0     0 9019 1398830 16 84  0  0  0
 8  0      0 6487428 118240 1292772    0    0     0     0 10191 1392312 16 84  0  0  0

r 列：就绪队列的长度已经到了 8，远远超过了系统 CPU 的个数 2，所以肯定会有大量的 CPU 竞争
us（user）和 sy（system）列：系统 CPU 使用率，也就是 sy 列高达 84%，说明 CPU 主要是被内核占用了
in  列：中断次数也上升到了 1 万左右，说明中断处理也是个潜在的问题

// 第三个终端再用 pidstat 来看一下， CPU 和进程上下文切换的情况
// 每隔1秒输出1组数据（需要 Ctrl+C 才结束）-w参数表示输出进程切换指标，而-u参数则表示输出CPU使用指标$ pidstat -w -u 1
CPU 使用率的升高果然是 sysbench 导致的，它的 CPU 使用率已经达到了 100%。但上下文切换则是来自其他进程，包括非自愿上下文切换频率最高的 pidstat  ，以及自愿上下文切换频率最高的内核线程 kworker 和 sshd

// pidstat 默认显示进程的指标数据，加上 -t 参数后，才会输出线程的指标,-wt 参数表示输出线程的上下文切换指标
pidstat -wt 1
虽然 sysbench 进程（也就是主线程）的上下文切换次数看起来并不多，但它的子线程的上下文切换次数却有很多

// 第三个终端里，  Ctrl+C 停止刚才的 pidstat 命令，然后运行下面的命令，观察中断的变化情况
watch -d cat /proc/interrupts
变化速度最快的是重调度中断（RES），这个中断类型表示，唤醒空闲状态的 CPU 来调度新的任务运行。这是多处理器系统（SMP）中，调度器用来分散任务到不同 CPU 的机制，通常也被称为处理器间中断（Inter-Processor Interrupts，IPI），这里的中断升高还是因为过多任务的调度问题，跟前面上下文切换次数的分析结果是一致的
```

如果系统的上下文切换次数比较稳定，那么从数百到一万以内，都应该算是正常的。但当上下文切换次数超过一万次，或者切换次数出现数量级的增长时，就很可能已经出现了性能问题。这时，还需要根据上下文切换的类型，再做具体分析：

* 自愿上下文切换变多了，说明进程都在等待资源，有可能发生了 I/O 等其他问题
* 非自愿上下文切换变多了，说明进程都在被强制调度，也就是都在争抢 CPU，说明 CPU 的确成了瓶颈
* 中断次数变多了，说明 CPU 被中断处理程序占用，需要查看 /proc/interrupts 文件来分析具体的中断类型

### 某个应用的CPU使用率居然达到100%，我该怎么办？

##### CPU 使用率

为了维护 CPU 时间，Linux 通过事先定义的节拍率（内核中表示为 HZ），触发时间中断，并使用全局变量 Jiffies 记录了开机以来的节拍数。每发生一次时间中断，Jiffies 的值就加 1

* 节拍率 HZ 是内核的可配选项，不同的系统可能设置不同数值，你可以通过查询 /boot/config 内核选项来查看它的配置值，用户空间程序并不能直接访问该变量

```
grep 'CONFIG_HZ=' /boot/config-$(uname -r)
CONFIG_HZ=250
```

* 为了方便用户空间程序，内核还提供了一个用户空间节拍率 USER_HZ，它总是固定为 100，也就是 1/100 秒

Linux 通过 /proc 虚拟文件系统，向用户空间提供了系统内部状态的信息，而 /proc/stat 提供的就是系统的 CPU 和任务统计信息

```
// 只保留各个CPU的数据
cat /proc/stat | grep ^cpu
cpu  280580 7407 286084 172900810 83602 0 583 0 0 0
cpu0 144745 4181 176701 86423902 52076 0 301 0 0 0
cpu1 135834 3226 109383 86476907 31525 0 282 0 0 0

每一列的数值含义：
user（通常缩写为 us），代表用户态 CPU 时间。注意，它不包括下面的 nice 时间，但包括了 guest 时间
nice（通常缩写为 ni），代表低优先级用户态 CPU 时间，也就是进程的 nice 值被调整为 1-19 之间时的 CPU 时间。这里注意，nice 可取值范围是 -20 到 19，数值越大，优先级反而越低
system（通常缩写为 sys），代表内核态 CPU 时间
idle（通常缩写为 id），代表空闲时间。注意，它不包括等待 I/O 的时间（iowait）
iowait（通常缩写为 wa），代表等待 I/O 的 CPU 时间
irq（通常缩写为 hi），代表处理硬中断的 CPU 时间
softirq（通常缩写为 si），代表处理软中断的 CPU 时间
steal（通常缩写为 st），代表当系统运行在虚拟机中的时候，被其他虚拟机占用的 CPU 时间
guest（通常缩写为 guest），代表通过虚拟化运行其他操作系统的时间，也就是运行虚拟机的 CPU 时间guest_nice（通常缩写为 gnice），代表以低优先级运行虚拟机的时间
```

CPU 使用率：除了空闲时间外的其他时间占总 CPU 时间的百分比，性能工具一般都会取间隔一段时间（比如 3 秒）的两次值，作差后，再计算出这段时间内的平均 CPU 使用率

Linux 给每个进程提供了运行情况的统计信息， /proc/[pid]/stat。这个文件包含的数据总共有 52 列的数据

对比一下 top 和 ps 这两个工具报告的 CPU 使用率，默认的结果很可能不一样，因为 top 默认使用 3 秒时间间隔，而 ps 使用的却是进程的整个生命周期

##### 怎么查看 CPU 使用率

* top：显示了系统总体的 CPU 和内存使用情况，以及各个进程的资源使用情况

```
// 默认每3秒刷新一次
top
top - 11:58:59 up 9 days, 22:47,  1 user,  load average: 0.03, 0.02, 0.00
Tasks: 123 total,   1 running,  72 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.3 us,  0.3 sy,  0.0 ni, 99.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  8169348 total,  5606884 free,   334640 used,  2227824 buff/cache
KiB Swap:        0 total,        0 free,        0 used.  7497908 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
    1 root      20   0   78088   9288   6696 S   0.0  0.1   0:16.83 systemd
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.05 kthreadd
    4 root       0 -20       0      0      0 I   0.0  0.0   0:00.00 kworker/0:0H

第三行显示CPU使用率，默认显示所有 CPU 平均值，按下数字 1 ，就可以切换到每个 CPU 的使用率
%CPU 列，表示进程的 CPU 使用率。它是用户态和内核态 CPU 使用率的总和
如果要细分进程的用户态 CPU 和内核态 CPU，需要使用pidstat
// 每隔1秒输出一组数据，共输出5组
$ pidstat 1 5
15:56:02      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
15:56:03        0     15006    0.00    0.99    0.00    0.00    0.99     1  dockerd

用户态 CPU 使用率 （%usr）
内核态 CPU 使用率（%system）
运行虚拟机 CPU 使用率（%guest）
等待 CPU 使用率（%wait）
总的 CPU 使用率（%CPU）
```

* ps：只显示每个进程的资源使用情况

##### CPU 使用率过高怎么办？

通过 top、ps、pidstat 等工具，你能够轻松找到 CPU 使用率较高（比如 100% ）的进程，但是无法定位是哪个函数，GDB 在调试程序错误方面很强大。但是并不适合在性能分析的早期应用。因为 GDB 调试程序的过程会中断程序运行，这在线上环境往往是不允许的。所以，GDB 只适合用在性能分析的后期，当你找到了出问题的大致函数后，线下再借助它来进一步调试函数内部的问题。适合在第一时间分析进程的 CPU 问题的是 perf

```
// perf top，类似于 top，它能够实时显示占用 CPU 时钟最多的函数或者指令，因此可以用来查找热点函数
perf top
Samples: 833  of event 'cpu-clock', Event count (approx.): 97742399
Overhead  Shared Object       Symbol
   7.28%  perf                [.] 0x00000000001f78a4
   4.72%  [kernel]            [k] vsnprintf
   4.32%  [kernel]            [k] module_get_kallsym
   3.65%  [kernel]            [k] _raw_spin_unlock_irqrestore
...

第一行包含三个数据，分别是采样数（Samples）、事件类型（event）和事件总数量（Event count）
第一列 Overhead ，是该符号的性能事件在所有采样中的比例，用百分比来表示
第二列 Shared ，是该函数或指令所在的动态共享对象（Dynamic Shared Object），如内核、进程名、动态链接库名、内核模块名等
第三列 Object ，是动态共享对象的类型。比如 [.] 表示用户空间的可执行程序、或者动态链接库，而 [k] 则表示内核空间
最后一列 Symbol 是符号名，也就是函数名。当函数名未知时，用十六进制的地址来表示

perf record 则提供了保存数据的功能，保存后的数据，需要用 perf report 解析展示，在实际使用中，会加上 -g 参数，开启调用关系的采样，方便根据调用链来分析性能问题
```

### 系统的 CPU 使用率很高，但为啥却找不到高 CPU 的应用？

```
top
...
%Cpu(s): 80.8 us, 15.1 sy,  0.0 ni,  2.8 id,  0.0 wa,  0.0 hi,  1.3 si,  0.0 st
...

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 6882 root      20   0    8456   5052   3884 S   2.7  0.1   0:04.78 docker-containe
 6947 systemd+  20   0   33104   3716   2340 S   2.7  0.0   0:04.92 nginx
 7494 daemon    20   0  336696  15012   7332 S   2.0  0.2   0:03.55 php-fpm
 7495 daemon    20   0  336696  15160   7480 S   2.0  0.2   0:03.55 php-fpm
10547 daemon    20   0  336696  16200   8520 S   2.0  0.2   0:03.13 php-fpm
10155 daemon    20   0  336696  16200   8520 S   1.7  0.2   0:03.12 php-fpm
10552 daemon    20   0  336696  16200   8520 S   1.7  0.2   0:03.12 php-fpm
15006 root      20   0 1168608  66264  37536 S   1.0  0.8   9:39.51 dockerd
 4323 root      20   0       0      0      0 I   0.3  0.0   0:00.87 kworker/u4:1
...
```

CPU 使用率最高的进程只不过才 2.7%，看起来并不高。然而，再看系统 CPU 使用率（ %Cpu ）这一行，系统的整体 CPU 使用率是比较高的：用户 CPU 使用率（us）已经到了 80%，用pidstat来分析进程的 CPU 使用情况

```
// 间隔1秒输出一组数据（按Ctrl+C结束）
pidstat 1
...
04:36:24      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
04:36:25        0      6882    1.00    3.00    0.00    0.00    4.00     0  docker-containe
04:36:25      101      6947    1.00    2.00    0.00    1.00    3.00     1  nginx
04:36:25        1     14834    1.00    1.00    0.00    1.00    2.00     0  php-fpm
04:36:25        1     14835    1.00    1.00    0.00    1.00    2.00     0  php-fpm
04:36:25        1     14845    0.00    2.00    0.00    2.00    2.00     1  php-fpm
04:36:25        1     14855    0.00    1.00    0.00    1.00    1.00     1  php-fpm
04:36:25        1     14857    1.00    2.00    0.00    1.00    3.00     0  php-fpm
04:36:25        0     15006    0.00    1.00    0.00    0.00    1.00     0  dockerd
04:36:25        0     15801    0.00    1.00    0.00    0.00    1.00     1  pidstat
04:36:25        1     17084    1.00    0.00    0.00    2.00    1.00     0  stress
04:36:25        0     31116    0.00    1.00    0.00    0.00    1.00     0  atopacctd
...
```

所有进程的 CPU 使用率也都不高，继续观察top一会

```
top
top - 04:58:24 up 14 days, 15:47,  1 user,  load average: 3.39, 3.82, 2.74
Tasks: 149 total,   6 running,  93 sleeping,   0 stopped,   0 zombie
%Cpu(s): 77.7 us, 19.3 sy,  0.0 ni,  2.0 id,  0.0 wa,  0.0 hi,  1.0 si,  0.0 st
KiB Mem :  8169348 total,  2543916 free,   457976 used,  5167456 buff/cache
KiB Swap:        0 total,        0 free,        0 used.  7363908 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 6947 systemd+  20   0   33104   3764   2340 S   4.0  0.0   0:32.69 nginx
 6882 root      20   0   12108   8360   3884 S   2.0  0.1   0:31.40 docker-containe
15465 daemon    20   0  336696  15256   7576 S   2.0  0.2   0:00.62 php-fpm
15466 daemon    20   0  336696  15196   7516 S   2.0  0.2   0:00.62 php-fpm
15489 daemon    20   0  336696  16200   8520 S   2.0  0.2   0:00.62 php-fpm
 6948 systemd+  20   0   33104   3764   2340 S   1.0  0.0   0:00.95 nginx
15006 root      20   0 1168608  65632  37536 S   1.0  0.8   9:51.09 dockerd
15476 daemon    20   0  336696  16200   8520 S   1.0  0.2   0:00.61 php-fpm
15477 daemon    20   0  336696  16200   8520 S   1.0  0.2   0:00.61 php-fpm
24340 daemon    20   0    8184   1616    536 R   1.0  0.0   0:00.01 stress
24342 daemon    20   0    8196   1580    492 R   1.0  0.0   0:00.01 stress
24344 daemon    20   0    8188   1056    492 R   1.0  0.0   0:00.01 stress
24347 daemon    20   0    8184   1356    540 R   1.0  0.0   0:00.01 stress
...
```

请求的进程是5个，而处于running的进程有6个， pidstat 来分析这几个进程，并且使用 -p 选项指定进程的 PID

```
pidstat -p 24344

16:14:55      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
```

没有任何输出

```
ps aux | grep 24344
root      9628  0.0  0.0  14856  1096 pts/0    S+   16:15   0:00 grep --color=auto 24344
```

还是没有任何输出，原来这个进程已经不存在了，所以 pidstat 就没有任何输出。既然进程都没了，那性能问题应该也没了吧？我们再用 top 命令确认一下

```
top
...
%Cpu(s): 80.9 us, 14.9 sy,  0.0 ni,  2.8 id,  0.0 wa,  0.0 hi,  1.3 si,  0.0 st
...

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 6882 root      20   0   12108   8360   3884 S   2.7  0.1   0:45.63 docker-containe
 6947 systemd+  20   0   33104   3764   2340 R   2.7  0.0   0:47.79 nginx
 3865 daemon    20   0  336696  15056   7376 S   2.0  0.2   0:00.15 php-fpm
 6779 daemon    20   0    8184   1112    556 R   0.3  0.0   0:00.01 stress
...
```

结果还跟原来一样，用户 CPU 使用率还是高达 80.9%，stress 进程的 PID 跟前面不一样了，原来的 PID 24344 不见了，现在的是 6779

要么是这些进程在不停地重启，要么就是全新的进程，这无非也就两个原因：

* 第一个原因，进程在不停地崩溃重启，比如因为段错误、配置错误等等，这时，进程在退出后可能又被监控系统自动重启了
* 第二个原因，这些进程都是短时进程，也就是在其他应用内部通过 exec 调用的外面命令。这些命令一般都只运行很短的时间就会结束，你很难用 top 这种间隔时间比较长的工具发现

要想继续分析下去，还得找到它们的父进程

```
pstree | grep stress
        |-docker-containe-+-php-fpm-+-php-fpm---sh---stress
        |         |-3*[php-fpm---sh---stress---stress]
```

在这个案例中，使用了 top、pidstat、pstree 等工具分析了系统 CPU 使用率高的问题，并发现 CPU 升高是短时进程 stress 导致的，但是整个分析过程还是比较复杂的。对于这类问题更好的方法监控是execsnoop：

分析工具：

* execsnoop：一个专为短时进程设计的工具。它通过 ftrace 实时监控进程的 exec() 行为，并输出短时进程的基本信息，包括进程 PID、父进程 PID、命令行参数以及执行的结果

  比如，用 execsnoop 监控上述案例，就可以直接得到 stress 进程的父进程 PID 以及它的命令行参数，并可以发现大量的 stress 进程在不停启动：

```
// 按 Ctrl+C 结束
$ execsnoop
PCOMM            PID    PPID   RET ARGS
sh               30394  30393    0
stress           30396  30394    0 /usr/local/bin/stress -t 1 -d 1
sh               30398  30393    0
stress           30399  30398    0 /usr/local/bin/stress -t 1 -d 1
sh               30402  30400    0
stress           30403  30402    0 /usr/local/bin/stress -t 1 -d 1
sh               30405  30393    0
stress           30407  30405    0 /usr/local/bin/stress -t 1 -d 1
...
```

### 怎么理解Linux软中断？

中断其实是一种异步的事件处理机制，可以提高系统的并发处理能力。由于中断处理程序会打断其他进程的运行，所以，为了减少对正常进程运行调度的影响，中断处理程序就需要尽可能快地运行。中断处理程序在响应中断时，还会临时关闭中断。这就会导致上一次中断处理完成之前，其他中断都不能响应，也就是说中断有可能会丢失

##### 软中断

为了解决中断处理程序执行过长和中断丢失的问题，Linux 将中断处理过程分成了两个阶段：

* 上半部：用来快速处理中断，在中断禁止模式下运行，主要处理跟硬件紧密相关的或时间敏感的工作。直接处理硬件请求，即硬中断，特点是快速执行，如键盘、鼠标的输入、硬盘的读取写入、网卡有数据

* 下半部：用来延迟处理上半部未完成的工作，通常以内核线程的方式运行。由内核触发，即软中断，特点是延迟执行，如网络收发、定时器、内核调度和 RCU 锁（Read-Copy Update 的缩写）

##### 查看软中断和内核线程

* /proc/softirqs 提供了软中断的运行情况
* /proc/interrupts 提供了硬中断的运行情况

```

cat /proc/softirqs
                    CPU0       CPU1
          HI:          0          0
       TIMER:     811613    1972736
      NET_TX:         49          7
      NET_RX:    1136736    1506885
       BLOCK:          0          0
    IRQ_POLL:          0          0
     TASKLET:     304787       3691
       SCHED:     689718    1897539
     HRTIMER:          0          0
         RCU:    1330771    1354737
```

软中断实际上是以内核线程的方式运行的，每个 CPU 都对应一个软中断内核线程，叫做  ksoftirqd/CPU 编号

```
ps aux | grep softirq
root         7  0.0  0.0      0     0 ?        S    Oct10   0:01 [ksoftirqd/0]
root        16  0.0  0.0      0     0 ?        S    Oct10   0:01 [ksoftirqd/1]
```

### 系统中出现大量不可中断进程和僵尸进程怎么办？

当碰到无法解释的 CPU 使用率问题时，先要检查一下是不是短时应用在捣鬼。短时应用的运行时间比较短，很难在 top 或者 ps 这类展示系统概要和进程快照的工具中发现，需要使用记录事件的工具来配合诊断，比如 execsnoop 或者 perf top

##### 进程状态

```
top
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
28961 root      20   0   43816   3148   4040 R   3.2  0.0   0:00.01 top
  620 root      20   0   37280  33676    908 D   0.3  0.4   0:00.01 app
    1 root      20   0  160072   9416   6752 S   0.0  0.1   0:37.64 systemd
 1896 root      20   0       0      0      0 Z   0.0  0.0   0:00.00 devapp
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.10 kthreadd
    4 root       0 -20       0      0      0 I   0.0  0.0   0:00.00 kworker/0:0H
    6 root       0 -20       0      0      0 I   0.0  0.0   0:00.00 mm_percpu_wq
    7 root      20   0       0      0      0 S   0.0  0.0   0:06.37 ksoftirqd/0
```

* R 是 Running 或 Runnable 的缩写，表示进程在 CPU 的就绪队列中，正在运行或者正在等待运行
* D 是 Disk Sleep 的缩写，不可中断状态睡眠（Uninterruptible Sleep），一般表示进程正在跟硬件交互，并且交互过程不允许被其他进程或中断打断
* Z 是 Zombie 的缩写，表示僵尸进程，也就是进程实际上已经结束了，但是父进程还没有回收它的资源
* S 是 Interruptible Sleep 的缩写，可中断状态睡眠，表示进程因为等待某个事件而被系统挂起。当进程等待的事件发生时，它会被唤醒并进入 R 状态
* I 是 Idle 的缩写，空闲状态，用在不可中断睡眠的内核线程上。硬件交互导致的不可中断进程用 D 表示，但对某些内核线程来说，它们有可能实际上并没有任何负载，用 Idle 正是为了区分这种情况。要注意，D 状态的进程会导致平均负载升高， I 状态的进程却不会
*  T 或者 t，Stopped 或 Traced 的缩写，表示进程处于暂停或者跟踪状态。向一个进程发送 SIGSTOP 信号，它就会因响应这个信号变成暂停状态（Stopped）；再向它发送 SIGCONT 信号，进程又会恢复运行（如果进程是终端里直接启动的，则需要你用 fg 命令，恢复到前台运行）
*  X， Dead 的缩写，表示进程已经消亡，不会在 top 或者 ps 命令中看到它

如果系统或硬件发生了故障，进程可能会在不可中断状态保持很久，甚至导致系统中出现大量不可中断进程。这时，你就得注意下，系统是不是出现了 I/O 等性能问题。

正常情况下，当一个进程创建了子进程后，应该通过系统调用 wait() 或者 waitpid() 等待子进程结束，回收子进程的资源；而子进程在结束时，会向它的父进程发送 SIGCHLD 信号，父进程还可以注册 SIGCHLD 信号的处理函数，异步回收资源。如果父进程没这么做，或是子进程执行太快，父进程还没来得及处理子进程状态，子进程就已经提前退出，那这时的子进程就会变成僵尸进程。通常，僵尸进程持续的时间都比较短，在父进程回收它的资源后就会消亡；或者在父进程退出后，由 init 进程回收后也会消亡。一旦父进程没有处理子进程的终止，还一直保持运行状态，那么子进程就会一直处于僵尸状态。大量的僵尸进程会用尽 PID 进程号，导致新进程不能创建，所以这种情况一定要避免

##### 案例分析

```
ps aux | grep /app
root      4009  0.0  0.0   4376  1008 pts/0    Ss+  05:51   0:00 /app
root      4287  0.6  0.4  37280 33660 pts/0    D+   05:54   0:00 /app
root      4288  0.6  0.4  37280 33668 pts/0    D+   05:54   0:00 /app
s 表示这个进程是一个会话的领导进程，而 + 表示前台进程组
```

* 进程组：表示一组相互关联的进程，比如每个子进程都是父进程所在组的成员
* 会话：指共享同一个控制终端的一个或多个进程组

```
// 按下数字 1 切换到所有 CPU 的使用情况，观察一会儿按 Ctrl+C 结束
$ top
top - 05:56:23 up 17 days, 16:45,  2 users,  load average: 2.00, 1.68, 1.39
Tasks: 247 total,   1 running,  79 sleeping,   0 stopped, 115 zombie
%Cpu0  :  0.0 us,  0.7 sy,  0.0 ni, 38.9 id, 60.5 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  :  0.0 us,  0.7 sy,  0.0 ni,  4.7 id, 94.6 wa,  0.0 hi,  0.0 si,  0.0 st
...

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 4340 root      20   0   44676   4048   3432 R   0.3  0.0   0:00.05 top
 4345 root      20   0   37280  33624    860 D   0.3  0.0   0:00.01 app
 4344 root      20   0   37280  33624    860 D   0.3  0.4   0:00.01 app
    1 root      20   0  160072   9416   6752 S   0.0  0.1   0:38.59 systemd
...

1. 第一行的平均负载（ Load Average），平均负载正在升高；而 1 分钟内的平均负载已经达到系统的 CPU 个数，说明系统很可能已经有了性能瓶颈
2. 第二行的 Tasks，僵尸进程比较多，而且还在不停增加，说明有子进程在退出时没被清理
3. 两个 CPU 的使用率情况，iowait 分别是 60.5% 和 94.6%，好像有点儿不正常
4. 最后再看每个进程的情况， CPU 使用率最高的进程只有 0.3%，看起来并不高；但有两个进程处于 D 状态，它们可能在等待 I/O，但光凭这里并不能确定是它们导致了 iowait 升高
```

##### iowait 分析

分析工具：

* dstat：可以同时查看 CPU 和 I/O 这两种资源的使用情况，便于对比分析

```
// 间隔1秒输出10组数据
dstat 1 10
You did not select any stats, using -cdngy by default.
--total-cpu-usage-- -dsk/total- -net/total- ---paging-- ---system--
usr sys idl wai stl| read  writ| recv  send|  in   out | int   csw
  0   0  96   4   0|1219k  408k|   0     0 |   0     0 |  42   885
  0   0   2  98   0|  34M    0 | 198B  790B|   0     0 |  42   138
  0   0   0 100   0|  34M    0 |  66B  342B|   0     0 |  42   135
  0   0  84  16   0|5633k    0 |  66B  342B|   0     0 |  52   177
  0   3  39  58   0|  22M    0 |  66B  342B|   0     0 |  43   144
  0   0   0 100   0|  34M    0 | 200B  450B|   0     0 |  46   147
  0   0   2  98   0|  34M    0 |  66B  342B|   0     0 |  45   134
  0   0   0 100   0|  34M    0 |  66B  342B|   0     0 |  39   131
  0   0  83  17   0|5633k    0 |  66B  342B|   0     0 |  46   168
  0   3  39  59   0|  22M    0 |  66B  342B|   0     0 |  37   134
```

每当 iowait 升高（wai）时，磁盘的读请求（read）都会很大。这说明 iowait 的升高跟磁盘的读请求有关，很可能就是磁盘读导致

```
// 观察一会儿按 Ctrl+C 结束
top
...
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 4340 root      20   0   44676   4048   3432 R   0.3  0.0   0:00.05 top
 4345 root      20   0   37280  33624    860 D   0.3  0.0   0:00.01 app
 4344 root      20   0   37280  33624    860 D   0.3  0.4   0:00.01 app
...
```

有两个 D 状态的进程，PID 分别是 4344 和 4345

```
// -d 展示 I/O 统计数据，-p 指定进程号，间隔 1 秒输出 3 组数据
pidstat -d -p 4344 1 3
06:38:50      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:38:51        0      4344      0.00      0.00      0.00       0  app
06:38:52        0      4344      0.00      0.00      0.00       0  app
06:38:53        0      4344      0.00      0.00      0.00       0  app

kB_rd: 表示每秒读的 KB 数 
kB_wr: 表示每秒写的 KB 数
iodelay: 表示 I/O 的延迟（单位是时钟周期）
kB_ccwr: 表示在执行时写入磁盘被取消的数量
它们都是 0，表示此时没有任何的读写，说明问题不是 4344 进程导致的
```

使用 pidstat，但这次去掉进程号，干脆就来观察所有进程的 I/O 使用情况

```
// 间隔 1 秒输出多组数据 (这里是 20 组)
pidstat -d 1 20
...
06:48:46      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:47        0      4615      0.00      0.00      0.00       1  kworker/u4:1
06:48:47        0      6080  32768.00      0.00      0.00     170  app
06:48:47        0      6081  32768.00      0.00      0.00     184  app

06:48:47      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:48        0      6080      0.00      0.00      0.00     110  app

06:48:48      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:49        0      6081      0.00      0.00      0.00     191  app

06:48:49      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command

06:48:50      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:51        0      6082  32768.00      0.00      0.00       0  app
06:48:51        0      6083  32768.00      0.00      0.00       0  app

06:48:51      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:52        0      6082  32768.00      0.00      0.00     184  app
06:48:52        0      6083  32768.00      0.00      0.00     175  app

06:48:52      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:53        0      6083      0.00      0.00      0.00     105  app
...
```

确定是app 进程在进行磁盘读，进程想要访问磁盘，就必须使用系统调用，需要找出  app 进程的系统调用。strace  正是最常用的跟踪进程系统调用的工具。所以，从 pidstat 的输出中拿到进程的 PID 号，比如 6082，然后在终端中运行 strace 命令，并用 -p 参数指定 PID 号

```
strace -p 6082
strace: attach: ptrace(PTRACE_SEIZE, 6082): Operation not permitted
```

如果已经是以root运行，则先检查一下进程的状态是否正常

```
ps aux | grep 6082
root      6082  0.0  0.0      0     0 pts/0    Z+   13:43   0:00 [app] <defunct>
```

发现6082已经变成僵尸了，没法儿继续分析它的系统调用，可以用  perf top 看看有没有新发现。或者在终端中运行 perf record，持续一会儿（例如 15 秒），然后按 Ctrl+C 退出，再运行 perf report 查看报告。app 的确在通过系统调用 sys_read() 读取数据。并且从 new_sync_read 和 blkdev_direct_IO  能看出，进程正在对磁盘进行直接读，也就是绕过了系统缓存，每个读请求都会从磁盘直接读，这就可以解释我们观察到的 iowait 升高了

##### 僵尸进程分析

```
// -a 表示输出命令行选项, p表PID, s表示指定进程的父进程
$ pstree -aps 3084
systemd,1
  └─dockerd,15006 -H fd://
      └─docker-containe,15024 --config /var/run/docker/containerd/containerd.toml
          └─docker-containe,3991 -namespace moby -workdir...
              └─app,4009
                  └─(app,3084)
```

3084 号进程的父进程是 4009，就是 app 应用，查看 app 应用程序的代码，看看子进程结束的处理是否正确，比如有没有调用 wait() 或 waitpid() ，抑或是，有没有注册 SIGCHLD 信号的处理函数

### 系统的软中断CPU使用率升高，该怎么办？

 Linux 中，每个 CPU 都对应一个软中断内核线程，名字是 ksoftirqd/CPU 编号。当软中断事件的频率过高时，内核线程也会因为 CPU 使用率过高而导致软中断处理不及时，进而引发网络收发延迟、调度缓慢等性能问题

分析工具：

* sar：是一个系统活动报告工具，既可以实时查看系统的当前活动，又可以配置保存和报告历史统计数据
* hping3：是一个可以构造 TCP/IP 协议数据包的工具，可以对系统进行安全审计、防火墙测试等
* tcpdump： 是一个常用的网络抓包工具，常用来分析各种网络问题

第二个终端模拟 Nginx 的客户端请求：

```
// -S参数表示设置TCP协议的SYN（同步序列号），-p表示目的端口为80, -i u100表示每隔100微秒发送一个网络帧
// 如果在实践过程中现象不明显，可以尝试把100调小，比如调成10甚至1
$ hping3 -S -p 80 -i u100 192.168.0.30
```

发现第一个终端反应变得很慢，通过top查看：

```
// top运行后按数字1切换到显示所有CPU
$ top
top - 10:50:58 up 1 days, 22:10,  1 user,  load average: 0.00, 0.00, 0.00
Tasks: 122 total,   1 running,  71 sleeping,   0 stopped,   0 zombie
%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni, 96.7 id,  0.0 wa,  0.0 hi,  3.3 si,  0.0 st
%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni, 95.6 id,  0.0 wa,  0.0 hi,  4.4 si,  0.0 st
...

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
    7 root      20   0       0      0      0 S   0.3  0.0   0:01.64 ksoftirqd/0
   16 root      20   0       0      0      0 S   0.3  0.0   0:01.97 ksoftirqd/1
 2663 root      20   0  923480  28292  13996 S   0.3  0.3   4:58.66 docker-containe
 3699 root      20   0       0      0      0 I   0.3  0.0   0:00.13 kworker/u4:0
 3708 root      20   0   44572   4176   3512 R   0.3  0.1   0:00.07 top
    1 root      20   0  225384   9136   6724 S   0.0  0.1   0:23.25 systemd
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.03 kthreadd
...

两个 CPU 的使用率虽然分别只有 3.3% 和 4.4%，但都用在了软中断上；而从进程列表上也可以看到，CPU 使用率最高的也是软中断进程 ksoftirqd。软中断有点可疑了
```

通过watch 监控 /proc/softirqs 文件内容的变化情况

```
 watch -d cat /proc/softirqs
                    CPU0       CPU1
          HI:          0          0
       TIMER:    1083906    2368646
      NET_TX:         53          9
      NET_RX:    1550643    1916776
       BLOCK:          0          0
    IRQ_POLL:          0          0
     TASKLET:     333637       3930
       SCHED:     963675    2293171
     HRTIMER:          0          0
         RCU:    1542111    1590625

NET_RX，也就是网络数据包接收软中断的变化速率最快。而其他几种类型的软中断，是保证 Linux 调度、时钟和临界区保护这些正常工作所必需的，所以它们有一定的变化倒是正常的
```

sar可以用来查看系统的网络收发情况，不仅可以观察网络收发的吞吐量（BPS，每秒收发的字节数），还可以观察网络收发的 PPS，即每秒收发的网络帧数

```
// -n DEV 表示显示网络收发的报告，间隔1秒输出一组数据
$ sar -n DEV 1
15:03:46        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
15:03:47         eth0  12607.00   6304.00    664.86    358.11      0.00      0.00      0.00      0.01
15:03:47      docker0   6302.00  12604.00    270.79    664.66      0.00      0.00      0.00      0.00
15:03:47           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
15:03:47    veth9f6bbcd   6302.00  12604.00    356.95    664.66      0.00      0.00      0.00      0.05

第一列：表示报告的时间
第二列：IFACE 表示网卡
第三、四列：rxpck/s 和 txpck/s 分别表示每秒接收、发送的网络帧数，也就是  PPS
第五、六列：rxkB/s 和 txkB/s 分别表示每秒接收、发送的千字节数，也就是  BPS

1. 对网卡 eth0 来说，每秒接收的网络帧数比较大，达到了 12607，而发送的网络帧数则比较小，只有 6304
2. docker0 和 veth9f6bbcd 的数据跟 eth0 基本一致，只是发送和接收相反，发送的数据较大而接收的数据较小。这是 Linux 内部网桥转发导致的，暂且不用深究，这是系统把 eth0 收到的包转发给 Nginx 服务即可
3.  eth0 ：接收的 PPS 比较大，达到 12607，而接收的 BPS 却很小，只有 664 KB。直观来看网络帧应该都是比较小的，664*1024/12607 = 54 字节，平均每个网络帧只有 54 字节，这是很小的网络帧，也就是通常所说的小包问题
```

第一个终端运行 tcpdump，通过 -i eth0 选项指定网卡 eth0，通过 tcp port 80 选项指定 TCP 协议的 80 端口

```
// -i eth0 只抓取eth0网卡，-n不解析协议名和主机名, tcp port 80表示只抓取tcp协议并且端口号为80的网络帧
tcpdump -i eth0 -n tcp port 80
15:11:32.678966 IP 192.168.0.2.18238 > 192.168.0.30.80: Flags [S], seq 458303614, win 512, length 0
...

1. 表示网络帧从 192.168.0.2 的 18238 端口发送到 192.168.0.30 的 80 端口，也就是从运行 hping3 机器的 18238 端口发送网络帧，目的为 Nginx 所在机器的 80 端口。Flags [S] 则表示这是一个 SYN 包
2. 现在可以确认，这就是从 192.168.0.2 这个地址发送过来的 SYN FLOOD 攻击
3. SYN FLOOD 问题最简单的解决方法，就是从交换机或者硬件防火墙中封掉来源 IP，这样 SYN FLOOD 网络帧就不会发送到服务器中
```

### 如何迅速分析出系统CPU的瓶颈在哪里？

##### CPU 性能指标

![img](https://static001.geekbang.org/resource/image/1e/07/1e66612e0022cd6c17847f3ab6989007.png)

* CPU使用率
  * 用户 CPU 使用率，包括用户态 CPU 使用率（user）和低优先级用户态 CPU 使用率（nice），表示 CPU 在用户态运行的时间百分比。用户 CPU 使用率高，通常说明有应用程序比较繁忙
  * 系统 CPU 使用率，表示 CPU 在内核态运行的时间百分比（不包括中断）。系统 CPU 使用率高，说明内核比较繁忙
  * 等待 I/O 的 CPU 使用率，通常也称为 iowait，表示等待 I/O 的时间百分比。iowait 高，通常说明系统与硬件设备的 I/O 交互时间比较长
  * 软中断和硬中断的 CPU 使用率，分别表示内核调用软中断处理程序、硬中断处理程序的时间百分比。它们的使用率高，通常说明系统发生了大量的中断。
  * 在虚拟化环境中会用到的窃取 CPU 使用率（steal）和客户 CPU 使用率（guest），分别表示被其他虚拟机占用的 CPU 时间百分比，和运行客户虚拟机的 CPU 时间百分比
* 平均负载（Load Average）：也就是系统的平均活跃进程数。它反应了系统的整体负载情况，包括过去 1 分钟、过去 5 分钟和过去 15 分钟的平均负载。理想情况下，平均负载等于逻辑 CPU 个数，表示每个 CPU 都恰好被充分利用。如果平均负载大于逻辑 CPU 个数，就表示负载比较重了
* 进程上下文切换：包括无法获取资源而导致的自愿上下文切换；被系统强制调度导致的非自愿上下文切换。上下文切换，本身是保证 Linux 正常运行的一项核心功能。但过多的上下文切换，会将原本运行进程的 CPU 时间，消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，缩短进程真正运行的时间，成为性能瓶颈
* CPU 缓存的命中率：CPU 缓存的速度介于 CPU 和内存之间，缓存的是热点的内存数据。根据不断增长的热点数据，这些缓存按照大小不同分为 L1、L2、L3 等三级缓存，其中 L1 和 L2 常用在单核中， L3 则用在多核中。从 L1 到 L3，三级缓存的大小依次增大，相应的，性能依次降低（当然比内存还是好得多）。而它们的命中率，衡量的是 CPU 缓存的复用情况，命中率越高，则表示性能越好

##### 性能工具

常见问题的分析步骤

1. 平均负载：先用 uptime查看了系统的平均负载；而在平均负载升高后，又用 mpstat 和 pidstat  ，分别观察了每个 CPU 和每个进程 CPU 的使用情况，进而找出了导致平均负载升高的进程
2. 上下文切换：先用 vmstat查看了系统的上下文切换次数和中断次数；然后通过 pidstat ，观察了进程的自愿上下文切换和非自愿上下文切换情况；最后通过 pidstat ，观察了线程的上下文切换情况，找出了上下文切换次数增多的根源
3. 进程 CPU 使用率升高：先用 top查看了系统和进程的 CPU 使用情况，发现 CPU 使用率升高的进程是 php-fpm；再用 perf top  ，观察 php-fpm 的调用链，最终找出 CPU 升高的根源
4. 系统的 CPU 使用率升高：先用  top 观察到了系统 CPU 升高，但通过 top 和 pidstat  ，却找不出高 CPU 使用率的进程。重新审视 top 的输出，又从 CPU 使用率不高但处于 Running 状态的进程入手，找出了可疑之处，最终通过  perf record 和 perf report ，发现原来是短时进程在捣鬼。对于短时进程的专门工具 execsnoop，它可以实时监控进程调用的外部命令
5. 不可中断进程和僵尸进程：先用 top 观察到了 iowait 升高的问题，并发现了大量的不可中断进程和僵尸进程；接着用 dstat 发现是这是由磁盘读导致的，于是又通过 pidstat 找出了相关的进程。但用 strace 查看进程系统调用却失败了，最终还是用  perf 分析进程调用链，才发现根源在于磁盘直接 I/O 
6. 软中断：先用 top 观察到，系统的软中断 CPU 使用率升高；接着查看 /proc/softirqs， 找到了几种变化速率较快的软中断；然后通过 sar 命令，发现是网络小包的问题，最后再用  tcpdump  ，找出网络帧的类型和来源，确定是一个 SYN FLOOD 攻击导致的

CPU性能指标的维度：

![img](https://static001.geekbang.org/resource/image/59/ec/596397e1d6335d2990f70427ad4b14ec.png)

CPU性能工具的维度：

![img](https://static001.geekbang.org/resource/image/b0/ca/b0c67a7196f5ca4cc58f14f959a364ca.png)

缩小排查范围，通常先运行几个支持指标较多的工具，如 top、vmstat 和 pidstat 

![img](https://static001.geekbang.org/resource/image/7a/17/7a445960a4bc0a58a02e1bc75648aa17.png)

